{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models import ResNet50\n",
    "from src.utils.common import UnCallBack\n",
    "from src.utils.evaluate import makeEval\n",
    "from src.utils.data_loaders import train_val_dataloader, test_dataloader, train_dataloader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonArguments( test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = ResNet50()\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "\n",
    "    eval_callback = makeEval(test_loader, loss_fn, device)\n",
    "\n",
    "    return {\"model\":model, \"optimizer\":optimizer, \"loss_fn\":loss_fn, \"eval\":eval_callback, \"device\":device}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport src.utils.common\n",
    "%aimport src.utils.evaluate\n",
    "%aimport src.train_importance_sampling\n",
    "%aimport src.train_rho_loss\n",
    "%aimport src.train_apricot\n",
    "%aimport src.train_schaul\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All learning algorithms are enclosed in functions to avoid memory problems. We avoid using global variables to allow the garbage collector to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper-bound and/or Loss\n",
    "\n",
    "To use Upper-bound method use use_loss_estimation = False, \\\n",
    "for Loss use use_loss_estimation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_importance_sampling import  train_full_upper_bound\n",
    "\n",
    "def upperBoundOrLoss(train_loader, test_loader, n_epochs = 50, use_loss_estimation = False):\n",
    "    \n",
    "    if use_loss_estimation:\n",
    "        print(\"upper-bound based algorithm\")\n",
    "    else:\n",
    "        print(\"loss based algorithm\") \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = ResNet50()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "\n",
    "    callback = UnCallBack( info_list = ['train_loss', 'train_acc', 'train_w_loss', 'val_loss', 'val_acc', 'train_uniform_cnt'])\n",
    "\n",
    "    eval_callback = makeEval(test_loader, loss_fn, device)\n",
    "\n",
    "    train_full_upper_bound( model, \n",
    "                train_loader, \n",
    "                loss_fn, \n",
    "                optimizer, \n",
    "                n_epochs = n_epochs, \n",
    "                eval = eval_callback, \n",
    "                callback = callback, \n",
    "                presample = 3, \n",
    "                tau_th = None,\n",
    "                use_loss_estimation = use_loss_estimation,\n",
    "                second_approach = True,\n",
    "                device= device)\n",
    "\n",
    "    if use_loss_estimation:\n",
    "        callback.save(\"callbacks/loss\")\n",
    "    else:\n",
    "        callback.save(\"callbacks/upper_bound\")\n",
    "\n",
    "    return callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHO-LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_rho_loss import train_full_rho_loss\n",
    "def rhoLoss(train_loader, test_loader, n_epochs = 50, train_irr_loader = None):\n",
    "    print(\"RHO-LOSS based algorithm\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = ResNet50()\n",
    "    model.to(device)\n",
    "\n",
    "    model_irr = ResNet50()\n",
    "    model_irr.to(device)\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "    optimizer_irr = torch.optim.Adam(model_irr.parameters(), lr = 1e-3 )\n",
    "\n",
    "    callback = UnCallBack( info_list = ['train_loss', 'train_acc', 'train_w_loss', 'val_loss', 'val_acc', 'train_uniform_cnt'])\n",
    "\n",
    "    eval_callback = makeEval(test_loader, loss_fn, device)\n",
    "\n",
    "    train_full_rho_loss(model, \n",
    "            model_irr,\n",
    "            train_loader, \n",
    "            train_irr_loader,\n",
    "            loss_fn, \n",
    "            optimizer, \n",
    "            optimizer_irr,\n",
    "            n_epochs=n_epochs, \n",
    "            eval=eval_callback, \n",
    "            callback=callback, \n",
    "            presample=3, \n",
    "            device = device)\n",
    "    \n",
    "    callback.save(\"callbacks/rho_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apricot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_apricot import train_full_apricot\n",
    "def apricot(train_loader, test_loader, n_epochs = 50):\n",
    "    print(\"apricot algorithm\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = ResNet50()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "\n",
    "    callback = UnCallBack( info_list = ['train_loss', 'train_acc', 'train_w_loss', 'val_loss', 'val_acc', 'train_uniform_cnt'])\n",
    "\n",
    "    train_full_apricot(\n",
    "        model, \n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_epochs = 50,\n",
    "        callback = callback,\n",
    "        device=device)\n",
    "    \n",
    "    callback.save(\"callbacks/apricot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schaul training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1\n",
    "from src.train_schaul import train_full_schaul, train_full_schaul2\n",
    "\n",
    "\n",
    "def schaul(train_loader, test_loader, n_epochs = 50):\n",
    "    print(\"schaul algorithm\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ResNet50()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "\n",
    "    callback = UnCallBack( info_list = ['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "\n",
    "    eval_callback = makeEval(test_loader, loss_fn, device)\n",
    "\n",
    "    train_full_schaul(model, \n",
    "                    train_loader, \n",
    "                    loss_fn, \n",
    "                    optimizer, \n",
    "                    n_epochs = n_epochs, \n",
    "                    eval = eval_callback, \n",
    "                    callback = callback, \n",
    "                    device = device)\n",
    "\n",
    "    callback.save(\"callbacks/schaul\")\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loshchilov training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_loshchilov import train_full_loshchilov\n",
    "\n",
    "def loshchilov(train_loader, test_loader, n_epochs = 50):\n",
    "    print(\"loshchilov algorithm\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = ResNet50()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n",
    "\n",
    "    callback = UnCallBack( info_list = ['train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "    eval_callback = makeEval(test_loader, loss_fn, device)\n",
    "\n",
    "    train_full_loshchilov(model, \n",
    "                        train_loader, \n",
    "                        loss_fn, \n",
    "                        optimizer, \n",
    "                        n_epochs=n_epochs, \n",
    "                        eval = eval_callback, \n",
    "                        callback=callback, \n",
    "                        device = device)\n",
    "\n",
    "    callback.save(\"callbacks/loshchilov\")\n",
    "\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1\n",
    "n_epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "train_loader = train_dataloader(batch_size=120)\n",
    "test_loader = test_dataloader(batch_size=120)\n",
    "\n",
    "# run upper-bound algorithm\n",
    "callback_upper_bound = upperBoundOrLoss(train_loader, test_loader, n_epochs)\n",
    "# run loss based algorithm\n",
    "callback_loss = upperBoundOrLoss(train_loader, test_loader, n_epochs, use_loss_estimation=True)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "train_irr_loader  = train_dataloader(batch_size=300, subset=0.25)\n",
    "# wee need indexes of data to use precomputed irreduseble loses \n",
    "train_loader  = train_dataloader(batch_size=300, index = True) \n",
    "\n",
    "# run rho-loss based algorithm\n",
    "callback_rho_loss = rhoLoss(train_loader, test_loader, n_epochs, train_irr_loader= train_irr_loader)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "train_loader = train_dataloader(batch_size=80)\n",
    "\n",
    "train_loader, test_loader = train_val_dataloader(batch_size=120)\n",
    "schaul_callback = schaul(train_loader, test_loader, n_epochs)\n",
    "\n",
    "\n",
    "train_loader = train_dataloader(batch_size=64)\n",
    "loshchilov_callback = loshchilov(train_loader, test_loader, n_epochs)\n",
    "\n",
    "\n",
    "# it is better to run it separately because \n",
    "# this algorithm wasn't optimised on memory usage \n",
    "#####################################################################\n",
    "#train_loader = train_dataloader(batch_size=120)\n",
    "#callback_apricot = apricot(train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to run it separately because \n",
    "# this algorithm wasn't optimised on memory usage \n",
    "#####################################################################\n",
    "train_loader = train_dataloader(batch_size=120)\n",
    "callback_apricot = apricot(train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to plot the logs\n",
    "\n",
    "since the callbacks are saved after each algorithm sucsessful call\n",
    "we can download information from the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import UnCallBack\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "methods = [\"upper_bound\",\"loss\",\"rho_loss\", \"schaul\", \"loshchilov_4\" ]\n",
    "\n",
    "callabacks = []\n",
    "for name in methods:\n",
    "    try: \n",
    "        callaback =  UnCallBack.load(f\"callbacks/{name}.pickle\")\n",
    "        callaback.meta[\"name\"] = name\n",
    "        callabacks.append(callaback)\n",
    "    except:\n",
    "        print(f\"can't fild the callbacks/{name}.pickle\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('ResNet50, train loss')\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "def smoth(x, N_ = 5):\n",
    "    return np.convolve([1.0/N_]*N_,x,\"valid\")\n",
    "\n",
    "length = min( len(i.train_loss) for i in callabacks)\n",
    "\n",
    "for callback in callabacks:\n",
    "    suffix = \"\"\n",
    "    if \"tau_th\" in callback.meta:\n",
    "        suffix = f', tau_th: {callback.meta[\"tau_th\"]}'\n",
    "    name = callback.meta[\"name\"]\n",
    "    min_loss = min(callback.train_loss[0:length])\n",
    "    y = smoth(callback.train_loss[0:length],1)\n",
    "    epochs = np.arange(length) + 1\n",
    "    plt.plot(epochs, y , label=f'{name}{suffix}, min: {min_loss:.4f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('ResNet50, test error')\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "for callback in callabacks:\n",
    "    suffix = \"\"\n",
    "    if \"tau_th\" in callback.meta:\n",
    "        suffix = f', tau_th: {callback.meta[\"tau_th\"]}'\n",
    "    name = callback.meta[\"name\"]\n",
    "\n",
    "    acc  = callback.val_acc[0:length]\n",
    "    min_loss = min(acc)\n",
    "    acc = smoth(acc,3)\n",
    "    #y = np.array(callback.val_acc)\n",
    "    epochs = np.arange(len(acc)) + 1\n",
    "    #plt.plot(epochs,1- np.array(callback.train_acc), label='Train'+suffix)\n",
    "    plt.plot(epochs, 1- acc , label=f'test {suffix}, max acc: {max(callback.val_acc):.3f} ' )\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
