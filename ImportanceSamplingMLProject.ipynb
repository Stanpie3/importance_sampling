{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ulqHQtCEW2CF"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/importance-sampling')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GZVfdZMw3B4Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10, MNIST, CIFAR100\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.functional import accuracy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_N4i8iV5J6T3"
      },
      "outputs": [],
      "source": [
        "normalize = transforms.Normalize(\n",
        "    mean=[0.4914, 0.4822, 0.4465],\n",
        "    std=[0.247, 0.2435, 0.2616],\n",
        ")\n",
        "\n",
        "def samplers(n, split_shuffle=True, val_size=0.1):\n",
        "    if split_shuffle:\n",
        "        idx = torch.randperm(n, generator=torch.Generator().manual_seed(0))\n",
        "    else:\n",
        "        idx = torch.arange(n)\n",
        "    split_idx = int((1.0 - val_size) * n)\n",
        "    train_sampler = SubsetRandomSampler(idx[:split_idx])\n",
        "    val_sampler = SubsetRandomSampler(idx[split_idx:])\n",
        "    return train_sampler, val_sampler\n",
        "\n",
        "def train_val_dataloader(root_dir='./cifar10', split_shuffle=True, val_size=0.1, batch_size=120):\n",
        "    train_dataset = CIFAR10(root=root_dir, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "    val_dataset = CIFAR10(root=root_dir, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "\n",
        "\n",
        "    train_sampler, val_sampler = samplers(len(train_dataset), split_shuffle, val_size)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "    val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=int(batch_size / 3))\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def test_dataloader(root_dir='./cifar10', batch_size=120):\n",
        "    test_dataset = CIFAR10(root=root_dir, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    return test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XKWRwU7F32-S"
      },
      "outputs": [],
      "source": [
        "def train_batch(model, x_batch, y_batch, loss_fn, optimizer, presample=3.0):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    batch_size = x_batch.shape[0]\n",
        "    selected_batch_size = int(batch_size / presample)\n",
        "    output = model(x_batch)\n",
        "    num_classes = output.shape[1]\n",
        "    with torch.no_grad():\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        one_hot_targets = F.one_hot(y_batch, num_classes=num_classes)\n",
        "        g_i_norm = torch.norm(probs - one_hot_targets, dim=-1).detach().cpu().numpy()\n",
        "    p_i = g_i_norm / np.sum(g_i_norm)\n",
        "    batch_indices = np.random.choice(np.arange(batch_size), size=selected_batch_size, replace=True, p=p_i)\n",
        "    selected_p_i = p_i[batch_indices]\n",
        "    loss = loss_fn(output, y_batch)\n",
        "    selected_loss = loss[batch_indices]\n",
        "    w_i = 1.0 / (batch_size * selected_p_i)\n",
        "    weighted_loss = (torch.tensor(w_i).to(selected_loss.device).detach() * selected_loss).mean()\n",
        "    weighted_loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss = loss.mean().cpu().item()\n",
        "    weighted_batch_loss = weighted_loss.mean().cpu().item()\n",
        "    max_p_i = np.max(p_i)\n",
        "    num_unique_points = np.unique(batch_indices).size\n",
        "    with torch.no_grad():\n",
        "        batch_acc_sum = (output.argmax(dim=1) == y_batch).sum().cpu().item()\n",
        "    return batch_loss, batch_acc_sum, weighted_batch_loss, max_p_i, num_unique_points, selected_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VarReductionCondition:\n",
        "\n",
        "    def __init__(self, vr_th=1.2, momentum=0.9):\n",
        "        self._vr_th = vr_th\n",
        "        self._vr = 0.0\n",
        "        self._previous_vr = 0.0\n",
        "        self._momentum = momentum\n",
        "\n",
        "    @property\n",
        "    def variance_reduction(self):\n",
        "        return self._vr\n",
        "\n",
        "    @property\n",
        "    def satisfied(self):\n",
        "        self._previous_vr = self._vr\n",
        "        return self._vr > self._vr_th\n",
        "\n",
        "    @property\n",
        "    def previously_satisfied(self):\n",
        "        return self._previous_vr > self._vr_th\n",
        "    \n",
        "    @property\n",
        "    def string(self):\n",
        "        return f\"vr={self._vr:.3f},  vr_th={self._vr_th:.3f}\"\n",
        "\n",
        "    def update(self, scores):\n",
        "        u = 1.0/len(scores)\n",
        "        S = scores.sum()\n",
        "        if S == 0:\n",
        "            g = np.array(u)\n",
        "        else:\n",
        "            g = scores/S\n",
        "        new_vr = 1.0 / np.sqrt(1 - ((g-u)**2).sum()/(g**2).sum())\n",
        "        self._vr = (\n",
        "            self._momentum * self._vr +\n",
        "            (1-self._momentum) * new_vr\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_g(output, y_batch  ):\n",
        "    num_classes = output.shape[1]\n",
        "    with torch.no_grad():\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        one_hot_targets = F.one_hot(y_batch, num_classes=num_classes)\n",
        "        g_i_norm = torch.norm(probs - one_hot_targets, dim=-1).detach().cpu().numpy()\n",
        "    return g_i_norm\n",
        "\n",
        "\n",
        "def train_batch_is(model,\n",
        "                x_batch, \n",
        "                y_batch, \n",
        "                loss_fn, \n",
        "                optimizer, \n",
        "                whole_dataset_size, \n",
        "                condition :VarReductionCondition, \n",
        "                presample = 3.0 ):\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    batch_size = x_batch.shape[0]\n",
        "\n",
        "    selected_batch_size = int(batch_size / presample)\n",
        "    \n",
        "    if condition.satisfied :\n",
        "        #print(\"condition satisfied\")\n",
        "        output = model(x_batch)\n",
        "        g_i_norm = get_g(output, y_batch  )\n",
        "        condition.update(g_i_norm)\n",
        "        N = batch_size\n",
        "        \n",
        "    else:\n",
        "        #print(\"condition not satisfied\")\n",
        "        g_i_norm = np.ones(batch_size)\n",
        "        N = whole_dataset_size\n",
        "\n",
        "    p_i = g_i_norm / np.sum(g_i_norm)\n",
        "    batch_indices = np.random.choice(np.arange(batch_size), size=selected_batch_size, replace=True, p=p_i)\n",
        "\n",
        "    selected_p_i = p_i[batch_indices]\n",
        "\n",
        "    if condition.previously_satisfied:\n",
        "        loss = loss_fn(output, y_batch)\n",
        "        selected_loss = loss[batch_indices]\n",
        "    else :\n",
        "        output = model(x_batch[batch_indices])\n",
        "        y_batch = y_batch[batch_indices]\n",
        "        condition.update( get_g(output, y_batch ) )\n",
        "        loss = loss_fn(output, y_batch)\n",
        "        selected_loss = loss\n",
        "\n",
        "        \n",
        "    w_i = 1.0 / (N * selected_p_i)\n",
        "\n",
        "    weighted_loss = (torch.tensor(w_i).to(selected_loss.device).detach() * selected_loss).mean()\n",
        "\n",
        "    weighted_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_loss = loss.mean().cpu().item()\n",
        "    weighted_batch_loss = weighted_loss.mean().cpu().item()\n",
        "    max_p_i = np.max(p_i)\n",
        "\n",
        "    num_unique_points = np.unique(batch_indices).size\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_acc_sum = (output.argmax(dim=1) == y_batch).sum().cpu().item()\n",
        "    \n",
        "    return batch_loss, batch_acc_sum, weighted_batch_loss, max_p_i, num_unique_points, selected_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pxY0TU4lyorv"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, loss_fn, optimizer, condition: VarReductionCondition, presample = 3):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0\n",
        "    epoch_weighted_loss = 0.0\n",
        "    epoch_max_p_i_s = []\n",
        "    epoch_num_unique_points_s = []\n",
        "    epoch_size = 0\n",
        "    epoch_weighted_size = 0\n",
        "    for i_batch, (X_batch, y_batch) in enumerate(dataloader):\n",
        "        #batch_loss, batch_acc_sum, weighted_batch_loss, max_p_i, num_unique_points, selected_batch_size = train_batch(model, X_batch.to(model.device), y_batch.to(model.device), loss_fn, optimizer)\n",
        "        (  batch_loss, \n",
        "           batch_acc_sum, \n",
        "           weighted_batch_loss, \n",
        "           max_p_i, \n",
        "           num_unique_points, \n",
        "           selected_batch_size) = train_batch_is(model, \n",
        "                                               X_batch.to(model.device), \n",
        "                                               y_batch.to(model.device), \n",
        "                                               loss_fn, \n",
        "                                               optimizer, \n",
        "                                               len(dataloader.dataset), \n",
        "                                               condition,\n",
        "                                               presample)\n",
        "        \n",
        "        epoch_size += len(X_batch)\n",
        "        epoch_weighted_size += selected_batch_size\n",
        "        epoch_loss += batch_loss * len(X_batch)\n",
        "        epoch_acc += batch_acc_sum\n",
        "        epoch_weighted_loss += weighted_batch_loss * selected_batch_size\n",
        "        epoch_max_p_i_s.append(max_p_i)\n",
        "        epoch_num_unique_points_s.append(num_unique_points)\n",
        "    epoch_loss /= epoch_size\n",
        "    epoch_acc /= epoch_size\n",
        "    epoch_weighted_loss /= epoch_weighted_size\n",
        "    return epoch_loss, epoch_acc, epoch_weighted_loss, epoch_max_p_i_s, epoch_num_unique_points_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "f-oBZz0aWgcG"
      },
      "outputs": [],
      "source": [
        "def train_full(model, train_dataloader, val_dataloader, loss_fn, optimizer, n_epochs, callback=None, presample=2, tau_th = None):\n",
        "    epochs = tqdm(range(n_epochs), desc='Epochs', leave=True)\n",
        "\n",
        "    large_batch = int( train_dataloader.batch_size)\n",
        "\n",
        "    # Compute the threshold using eq. 29 in\n",
        "    # https://arxiv.org/abs/1803.00942\n",
        "\n",
        "    presample = 2\n",
        "    B = large_batch\n",
        "    b = int( large_batch / presample)\n",
        "    tau_th = float(B + 3*b) / (3*b) if tau_th is None else   tau_th\n",
        "\n",
        "\n",
        "    condition = VarReductionCondition(tau_th)\n",
        "    print(condition.string)\n",
        "    for i_epoch in epochs:\n",
        "        epoch_loss, epoch_acc, epoch_weighted_loss, epoch_max_p_i_s, epoch_num_unique_points_s = train_epoch(model, train_dataloader, loss_fn, optimizer, condition, presample)\n",
        "        print(condition.string)\n",
        "        if callback is not None:\n",
        "            cb_dict = callback(model, val_dataloader, loss_fn, epoch_loss, epoch_acc, epoch_weighted_loss, epoch_max_p_i_s, epoch_num_unique_points_s)\n",
        "            epochs.set_postfix(cb_dict)\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "H4zSEp-ZY1Yw"
      },
      "outputs": [],
      "source": [
        "class CallBack:\n",
        "    def __init__(self, eval_fn, name=None):\n",
        "        self.eval_fn = eval_fn\n",
        "        self.train_losses = []\n",
        "        self.train_accs = []\n",
        "        self.train_w_losses = []\n",
        "        self.train_max_p_i = []\n",
        "        self.train_num_unique_points = []\n",
        "        self.val_losses = []\n",
        "        self.val_accs = []\n",
        "\n",
        "    def last_info(self):\n",
        "        return {'loss_train': f'{self.train_losses[-1]}',\n",
        "                'acc_train': f'{self.train_accs[-1]}',\n",
        "                'w_loss_train': f'{self.train_w_losses[-1]}',\n",
        "                'loss_val': f'{self.val_losses[-1]}',\n",
        "                'acc_val': f'{self.val_accs[-1]}',\n",
        "        }\n",
        "    def __call__(self, model, val_dataloader, loss_fn,\n",
        "                 epoch_loss=None, epoch_acc=None, epoch_weighted_loss=None, epoch_max_p_i_s=None, epoch_num_unique_points_s=None):\n",
        "        self.train_losses.append(epoch_loss)\n",
        "        self.train_accs.append(epoch_acc)\n",
        "        self.train_w_losses.append(epoch_weighted_loss)\n",
        "        self.train_max_p_i.append(epoch_max_p_i_s)\n",
        "        self.train_num_unique_points.append(epoch_num_unique_points_s)\n",
        "        loss_val, acc_val = self.eval_fn(model, val_dataloader, loss_fn)\n",
        "        self.val_losses.append(loss_val)\n",
        "        self.val_accs.append(acc_val)\n",
        "        return self.last_info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lIoX388SbM2B"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    logits = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            output = model(X_batch.to(model.device)).cpu()\n",
        "            logits.append(output)\n",
        "            targets.append(y_batch)\n",
        "    logits = torch.cat(logits)\n",
        "    targets = torch.cat(targets)\n",
        "    loss = loss_fn(logits, targets).mean().item()\n",
        "    acc = (logits.argmax(dim=1) == targets).sum().item() / len(targets)\n",
        "    return loss, acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RUtiAR0Tbqyj"
      },
      "outputs": [],
      "source": [
        "####ResNet\n",
        "#class BasicBlock(nn.Module):\n",
        "#    expansion = 1\n",
        "\n",
        "#    def __init__(self, in_planes, planes, stride=1):\n",
        "#        super(BasicBlock, self).__init__()\n",
        "#        self.activation = F.relu\n",
        "#        self.conv1 = nn.Conv2d(\n",
        "#            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "#        self.bn1 = nn.BatchNorm2d(planes)\n",
        "#        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "#                               stride=1, padding=1, bias=False)\n",
        "#        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "#        self.shortcut = nn.Sequential()\n",
        "#        if stride != 1 or in_planes != self.expansion*planes:\n",
        "#            self.shortcut = nn.Sequential(\n",
        "#                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "#                          kernel_size=1, stride=stride, bias=False),\n",
        "#                nn.BatchNorm2d(self.expansion*planes)\n",
        "#            )\n",
        "\n",
        "#    def forward(self, x):\n",
        "#        out = self.activation(self.bn1(self.conv1(x)))\n",
        "#        out = self.bn2(self.conv2(out))\n",
        "#        out += self.shortcut(x)\n",
        "#        out = self.activation(out)\n",
        "#        return out\n",
        "\n",
        "\n",
        "#class Bottleneck(nn.Module):\n",
        "#    expansion = 4\n",
        "\n",
        "#    def __init__(self, in_planes, planes, stride=1):\n",
        "#        super(Bottleneck, self).__init__()\n",
        "#        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "#        self.bn1 = nn.BatchNorm2d(planes)\n",
        "#        self.conv2 = nn.Conv2d(\n",
        "#            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "#        )\n",
        "#        self.bn2 = nn.BatchNorm2d(planes)\n",
        "#        self.conv3 = nn.Conv2d(\n",
        "#            planes, self.expansion * planes, kernel_size=1, bias=False\n",
        "#        )\n",
        "#        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "#        self.shortcut = nn.Sequential()\n",
        "#        if stride != 1 or in_planes != self.expansion * planes:\n",
        "#            self.shortcut = nn.Sequential(\n",
        "#                nn.Conv2d(\n",
        "#                    in_planes,\n",
        "#                    self.expansion * planes,\n",
        "#                    kernel_size=1,\n",
        "#                    stride=stride,\n",
        "#                    bias=False,\n",
        "#                ),\n",
        "#                nn.BatchNorm2d(self.expansion * planes),\n",
        "#            )\n",
        "\n",
        "#    def forward(self, x):\n",
        "#        out = F.relu(self.bn1(self.conv1(x)))\n",
        "#        out = F.relu(self.bn2(self.conv2(out)))\n",
        "#        out = self.bn3(self.conv3(out))\n",
        "#        out += self.shortcut(x)\n",
        "#        out = F.relu(out)\n",
        "#        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uGIg1q5hbwq6"
      },
      "outputs": [],
      "source": [
        "#class ResNet(nn.Module):\n",
        "#    def __init__(self, block, num_blocks, num_classes=10):\n",
        "#        super(ResNet, self).__init__()\n",
        "#        self.in_planes = 64\n",
        "#        self.activation = F.relu\n",
        "#        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "#                               stride=1, padding=1, bias=False)\n",
        "#        self.bn1 = nn.BatchNorm2d(64)\n",
        "#        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "#        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "#        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "#        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "#        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "\n",
        "#    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "#        strides = [stride] + [1]*(num_blocks-1)\n",
        "#        layers = []\n",
        "#        for stride in strides:\n",
        "#            layers.append(block(self.in_planes, planes, self.activation, stride))\n",
        "#            self.in_planes = planes * block.expansion\n",
        "#        return nn.Sequential(*layers)\n",
        "\n",
        "#    def forward(self, x):\n",
        "#        out = self.activation(self.bn1(self.conv1(x)))\n",
        "#        out = self.layer1(out)\n",
        "#        out = self.layer2(out)\n",
        "#        out = self.layer3(out)\n",
        "#        out = self.layer4(out)\n",
        "#        out = F.avg_pool2d(out, 4)\n",
        "#        out = out.view(out.size(0), -1)\n",
        "#        out = self.linear(out)\n",
        "#        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "eUTBIQf2by4m"
      },
      "outputs": [],
      "source": [
        "#def ResNet18():\n",
        "#    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "#def ResNet34():\n",
        "#    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "#def ResNet50():\n",
        "#    return ResNet(Bottleneck, [3, 4, 6, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fuyhwC0Ko0c4"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_planes,\n",
        "                    self.expansion * planes,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(self.expansion * planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, self.expansion * planes, kernel_size=1, bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_planes,\n",
        "                    self.expansion * planes,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(self.expansion * planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eMnobAkbcpyH"
      },
      "outputs": [],
      "source": [
        "### WideResNet\n",
        "\n",
        "class BasicBlockW(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
        "        super(BasicBlockW, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = drop_rate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth=28, num_classes=10, widen_factor=10, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = int((depth - 4) / 6)\n",
        "        block = BasicBlockW\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, n_channels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "        self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1, drop_rate)\n",
        "\n",
        "        self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2, drop_rate)\n",
        "\n",
        "        self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2, drop_rate)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(n_channels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(n_channels[3], num_classes)\n",
        "        self.nChannels = n_channels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b2a082ng909",
        "outputId": "95bb5ec9-c5e5-499c-a320-6a73ee1f28cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataloader, val_dataloader = train_val_dataloader()\n",
        "test_loader = test_dataloader(batch_size=120*2)\n",
        "\n",
        "#device = torch.device( 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "140f36be62884c6599ebbeb158c510fb",
            "deb6e2113cc447b086689fcbdad52c04",
            "b82a70b6df324aeaab2a9eb75bf85644",
            "200602909c07477cbcb532a0d65d5432",
            "334914dcb0c9465fa5d60661731228d4",
            "42960e386be646aebc002e0568c76564",
            "8b7e55b7f8fc42d589f1e65ab0cb5595",
            "cfebb3f379e44337a62adbc059dbe9ee",
            "ab493f44ff394bebb85e98de872d0f0d",
            "de9a1672a6fd47049e807687982874b9",
            "3caddfc99f464c39ae5cab28c6afb89a"
          ]
        },
        "id": "CYibWIl3fk3V",
        "outputId": "a0d3278b-d019-481a-d379-a8287955aeea"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet50()\n",
        "model.to(device)\n",
        "model.device = device\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "cb = CallBack(evaluate)\n",
        "train_full(model, train_dataloader, val_dataloader, loss_fn, optimizer, n_epochs=30, callback=cb, presample=2, tau_th = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiS4s5ZJgkm8"
      },
      "outputs": [],
      "source": [
        "loss, acc = evaluate(model, test_loader, loss_fn)\n",
        "print(f'ResNet50, test loss: {loss}')\n",
        "print(f'ResNet50, test accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67yEgznbgqG5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.title('ResNet50, train and validation loss')\n",
        "plt.xlabel('Number of epoch')\n",
        "plt.ylabel('Loss')\n",
        "epochs = np.arange(30) + 1\n",
        "plt.plot(epochs, cb.train_losses, label='Train')\n",
        "plt.plot(epochs, cb.train_w_losses, label='Train weighted')\n",
        "plt.plot(epochs, cb.val_losses, label='Validation')\n",
        "plt.legend()\n",
        "plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXMUyITWgv1c"
      },
      "outputs": [],
      "source": [
        "print(f'Best loss on train: {np.min(cb.train_losses)}, on {np.argmin(cb.train_losses) + 1} epoch')\n",
        "print(f'Best weighted loss on train: {np.min(cb.train_w_losses)}, on {np.argmin(cb.train_w_losses) + 1} epoch')\n",
        "print(f'Best loss on validation: {np.min(cb.val_losses)}, on {np.argmin(cb.val_losses) + 1} epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtqgLzixgwki"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.title('ResNet50, train and validation accuracy')\n",
        "plt.xlabel('Number of epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "epochs = np.arange(30) + 1\n",
        "plt.plot(epochs, cb.train_accs, label='Train')\n",
        "plt.plot(epochs, cb.val_accs, label='Validation')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwdbovOgyK_"
      },
      "outputs": [],
      "source": [
        "print(f'Best accuracy on train: {np.max(cb.train_accs)}, on {np.argmax(cb.train_accs) + 1} epoch')\n",
        "print(f'Best accuracy on validation: {np.max(cb.val_accs)}, on {np.argmax(cb.val_accs) + 1} epoch')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "140f36be62884c6599ebbeb158c510fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_deb6e2113cc447b086689fcbdad52c04",
              "IPY_MODEL_b82a70b6df324aeaab2a9eb75bf85644",
              "IPY_MODEL_200602909c07477cbcb532a0d65d5432"
            ],
            "layout": "IPY_MODEL_334914dcb0c9465fa5d60661731228d4"
          }
        },
        "200602909c07477cbcb532a0d65d5432": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9a1672a6fd47049e807687982874b9",
            "placeholder": "​",
            "style": "IPY_MODEL_3caddfc99f464c39ae5cab28c6afb89a",
            "value": " 1/30 [02:35&lt;1:15:13, 155.64s/it, loss_train=2.093855035463969, acc_train=0.26826666666666665, w_loss_train=2.093669682184855, loss_val=2.4236364364624023, acc_val=0.3442]"
          }
        },
        "334914dcb0c9465fa5d60661731228d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3caddfc99f464c39ae5cab28c6afb89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42960e386be646aebc002e0568c76564": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7e55b7f8fc42d589f1e65ab0cb5595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab493f44ff394bebb85e98de872d0f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b82a70b6df324aeaab2a9eb75bf85644": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfebb3f379e44337a62adbc059dbe9ee",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab493f44ff394bebb85e98de872d0f0d",
            "value": 1
          }
        },
        "cfebb3f379e44337a62adbc059dbe9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9a1672a6fd47049e807687982874b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb6e2113cc447b086689fcbdad52c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42960e386be646aebc002e0568c76564",
            "placeholder": "​",
            "style": "IPY_MODEL_8b7e55b7f8fc42d589f1e65ab0cb5595",
            "value": "Epochs:   3%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
