{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"60151a2289db43e2b456794e3f06fc1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54c3b0141aa540aba3e47dc4570fd4ef","IPY_MODEL_7c2acbf4d6bb44f387d0030bb5ff0f76","IPY_MODEL_a3309e941ae64ad6ade381716adfe1e4"],"layout":"IPY_MODEL_565268a023c046eabff4c9902475c8aa"}},"54c3b0141aa540aba3e47dc4570fd4ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_662b8b1e72c44beda0d8a18e6bba79b7","placeholder":"​","style":"IPY_MODEL_72984373f396446b921825b9e358b6d3","value":"Epochs:   0%"}},"7c2acbf4d6bb44f387d0030bb5ff0f76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_a043b49260ba41d294173a418e7616ea","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c271af61c5744a1ab3b4da5c2dba050","value":0}},"a3309e941ae64ad6ade381716adfe1e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72866ef973e045808060e98f6a75d56a","placeholder":"​","style":"IPY_MODEL_02c63ad4933741a2817453966685d2bc","value":" 0/40 [00:08&lt;?, ?it/s]"}},"565268a023c046eabff4c9902475c8aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"662b8b1e72c44beda0d8a18e6bba79b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72984373f396446b921825b9e358b6d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a043b49260ba41d294173a418e7616ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c271af61c5744a1ab3b4da5c2dba050":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72866ef973e045808060e98f6a75d56a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02c63ad4933741a2817453966685d2bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Stanpie3/importance_sampling\n!mv importance_sampling/* .\n!rm -r importance_sampling","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Klb1w4zPej8g","outputId":"f41ee66f-b3ea-4b80-d0d8-5e533d683e32","execution":{"iopub.status.busy":"2024-03-18T15:24:18.503266Z","iopub.execute_input":"2024-03-18T15:24:18.503962Z","iopub.status.idle":"2024-03-18T15:24:22.485862Z","shell.execute_reply.started":"2024-03-18T15:24:18.503917Z","shell.execute_reply":"2024-03-18T15:24:22.484565Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'importance_sampling'...\nremote: Enumerating objects: 130, done.\u001b[K\nremote: Counting objects: 100% (130/130), done.\u001b[K\nremote: Compressing objects: 100% (94/94), done.\u001b[K\nremote: Total 130 (delta 75), reused 69 (delta 32), pack-reused 0\u001b[K\nReceiving objects: 100% (130/130), 810.39 KiB | 5.83 MiB/s, done.\nResolving deltas: 100% (75/75), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom common_utils import Accumulator\nfrom torch_importance_sampling_tr import VarReductionCondition, get_g","metadata":{"id":"oneMTOhffyg8","execution":{"iopub.status.busy":"2024-03-18T15:24:22.487902Z","iopub.execute_input":"2024-03-18T15:24:22.488224Z","iopub.status.idle":"2024-03-18T15:24:24.437996Z","shell.execute_reply.started":"2024-03-18T15:24:22.488196Z","shell.execute_reply":"2024-03-18T15:24:24.436940Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def create_batches(p, N, k, b):\n    all_points = np.arange(N)\n    batches = []\n    for j in range(k):\n        batch = np.random.choice(all_points, size=b, replace=False, p=p)\n        batches.append(batch)\n        tmp = p[batch].sum()\n        p[batch] = 0\n        if j != k-1:\n            p[p>0] += tmp / p[p>0].shape[0]\n    return batches\n\ndef get_batches(loss, se, b):\n    sorted_arg_loss = (-loss).argsort()\n    N = loss.shape[0]\n    p = np.zeros(N)\n    for i in range(N):\n        p[sorted_arg_loss[i]] = 1.0 / np.exp(np.log(se) / N)**(i+1)\n    p /= p.sum()\n    print(f'Minimal probability {p.min()}, maximal probability {p.max()}')\n    return create_batches(p, N, N//b, b)","metadata":{"id":"0iw6kYFvxgO5","execution":{"iopub.status.busy":"2024-03-18T15:24:24.439465Z","iopub.execute_input":"2024-03-18T15:24:24.440442Z","iopub.status.idle":"2024-03-18T15:24:24.449676Z","shell.execute_reply.started":"2024-03-18T15:24:24.440404Z","shell.execute_reply":"2024-03-18T15:24:24.448663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train_batch_is(model,\n                x_batch,\n                y_batch,\n                loss_fn,\n                optimizer,\n                accumulator):\n\n    flag = False\n    model.train()\n    optimizer.zero_grad()\n\n    batch_size = x_batch.shape[0]\n    output = model(x_batch)\n    loss = loss_fn(output, y_batch)\n    loss = loss.mean()\n\n    loss.backward()\n\n    optimizer.step()\n\n    n = len(output)\n    with torch.no_grad():\n        batch_loss = loss.mean().cpu().item()\n        batch_acc_sum = (output.argmax(dim=1) == y_batch).sum().cpu().item()/n\n\n    accumulator.average(\n        train_loss = ( batch_loss, n),\n        train_acc = ( batch_acc_sum, n),\n        train_uniform_cnt = flag)","metadata":{"id":"BVGOrJC_0-bt","execution":{"iopub.status.busy":"2024-03-18T15:24:24.477601Z","iopub.execute_input":"2024-03-18T15:24:24.477840Z","iopub.status.idle":"2024-03-18T15:24:24.485081Z","shell.execute_reply.started":"2024-03-18T15:24:24.477819Z","shell.execute_reply":"2024-03-18T15:24:24.484064Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_full(model, train_dataloader, loss_fn, optimizer, n_epochs, eval = None, callback=None):\n    epochs = tqdm(range(n_epochs), desc='Epochs', leave=True)\n    X = torch.tensor(train_dataloader.dataset.data, dtype=torch.float32).transpose(1, -1)\n    y = torch.tensor(train_dataloader.dataset.targets).long()\n    batch_size = int(train_dataloader.batch_size)\n    # n_batches = len(train_dataloader)\n\n    if callback :\n        callback.setMeta(\n            large_batch = batch_size,\n            n_epochs = n_epochs)\n    n_batches = y.shape[0] // batch_size\n    losses = torch.zeros(n_batches * batch_size)\n    se_0, se_end = 10.0**2, 1.0\n    se = se_0\n    for i_epoch in epochs:\n        accum = Accumulator()\n        batch_indices = get_batches(np.abs(losses), se, batch_size)\n        print(len(batch_indices), len(batch_indices[0]))\n        for batch_idx in range(len(batch_indices)):\n            train_batch_is( model,\n                            X[batch_indices[batch_idx]].to(model.device),\n                            y[batch_indices[batch_idx]].to(model.device),\n                            loss_fn,\n                            optimizer,\n                            accum)\n        model.eval()\n        test_loss = 0\n        correct = 0\n\n        with torch.no_grad():\n            losses = torch.zeros(n_batches * batch_size)\n            for batch_idx in range(len(batch_indices)):\n                output = model(X[batch_indices[batch_idx]].to(model.device))\n                losses[batch_indices[batch_idx]] = loss_fn(output, y[batch_indices[batch_idx]].to(model.device)).cpu()\n        se = se_0 * np.exp(np.log(se_end/se_0)/n_epochs) ** i_epoch\n        print(losses)\n        if callback :\n            val_scores = eval(model) if eval else {}\n            cb_dict = callback(**accum.getAll(), **val_scores)\n            epochs.set_postfix(cb_dict)","metadata":{"id":"RfMjBHfZFy3w","execution":{"iopub.status.busy":"2024-03-18T15:33:01.460258Z","iopub.execute_input":"2024-03-18T15:33:01.461148Z","iopub.status.idle":"2024-03-18T15:33:01.473446Z","shell.execute_reply.started":"2024-03-18T15:33:01.461118Z","shell.execute_reply":"2024-03-18T15:33:01.472577Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, loss_fn):\n    model.eval()\n    logits = []\n    targets = []\n    with torch.no_grad():\n        for X_batch, y_batch in dataloader:\n            output = model(X_batch.to(model.device)).cpu()\n            logits.append(output)\n            targets.append(y_batch)\n    logits = torch.cat(logits)\n    targets = torch.cat(targets)\n    loss = loss_fn(logits, targets).mean().item()\n    acc = (logits.argmax(dim=1) == targets).sum().item() / len(targets)\n    return loss, acc","metadata":{"id":"iZnaoj_PvJUH","execution":{"iopub.status.busy":"2024-03-18T15:33:02.982367Z","iopub.execute_input":"2024-03-18T15:33:02.982763Z","iopub.status.idle":"2024-03-18T15:33:02.990923Z","shell.execute_reply.started":"2024-03-18T15:33:02.982731Z","shell.execute_reply":"2024-03-18T15:33:02.990007Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from data_loaders import train_val_dataloader, test_dataloader\n\ntrain_dataloader, val_dataloader = train_val_dataloader(batch_size=64)\ntest_loader = test_dataloader(batch_size=64)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1tQZ0JUvJ4y","outputId":"48258124-2500-452f-9757-2e312db56915","execution":{"iopub.status.busy":"2024-03-18T15:33:04.045344Z","iopub.execute_input":"2024-03-18T15:33:04.046413Z","iopub.status.idle":"2024-03-18T15:33:06.577560Z","shell.execute_reply.started":"2024-03-18T15:33:04.046374Z","shell.execute_reply":"2024-03-18T15:33:06.576523Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"from models import ResNet50\nfrom common_utils import UnCallBack\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel = ResNet50()\nmodel.to(device)\nmodel.device = device\n\nloss_fn = nn.CrossEntropyLoss(reduction='none')\noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 )\n\ncallback = UnCallBack( info_list = ['train_loss', 'train_acc', 'train_w_loss', 'val_loss', 'val_acc', 'train_uniform_cnt'])\n\ndef eval_callback(model):\n    loss, acc = evaluate(model, val_dataloader, loss_fn)\n    return {\"val_loss\": loss, \"val_acc\": acc}\n\ntrain_full(model, train_dataloader, loss_fn, optimizer, n_epochs=40, eval=eval_callback, callback=callback)\ncallback.save(\"callback\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403,"referenced_widgets":["60151a2289db43e2b456794e3f06fc1a","54c3b0141aa540aba3e47dc4570fd4ef","7c2acbf4d6bb44f387d0030bb5ff0f76","a3309e941ae64ad6ade381716adfe1e4","565268a023c046eabff4c9902475c8aa","662b8b1e72c44beda0d8a18e6bba79b7","72984373f396446b921825b9e358b6d3","a043b49260ba41d294173a418e7616ea","0c271af61c5744a1ab3b4da5c2dba050","72866ef973e045808060e98f6a75d56a","02c63ad4933741a2817453966685d2bc"]},"id":"IzdPdM4WvMzL","outputId":"8950b12d-7c76-4468-be74-48b89090a79f","execution":{"iopub.status.busy":"2024-03-18T15:33:06.579251Z","iopub.execute_input":"2024-03-18T15:33:06.579636Z","iopub.status.idle":"2024-03-18T15:50:32.502438Z","shell.execute_reply.started":"2024-03-18T15:33:06.579602Z","shell.execute_reply":"2024-03-18T15:50:32.501136Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197b6ecc8cb74bfdbf6e450a1be4fb58"}},"metadata":{}},{"name":"stdout","text":"torch.Size([50000, 3, 32, 32]) torch.Size([50000])\n704\nMinimal probability 9.306780869542679e-07, maximal probability 9.305923448407336e-05\n781 64\ntorch.Size([49984])\ntensor([0.2480, 0.3404, 0.5616,  ..., 0.5887, 0.6187, 0.6015])\nMinimal probability 9.306780869542681e-07, maximal probability 9.305923448407337e-05\n781 64\ntorch.Size([49984])\ntensor([0.0385, 0.3548, 0.1676,  ..., 2.1717, 0.5843, 0.5424])\nMinimal probability 1.0193872709318126e-06, maximal probability 9.084482523682283e-05\n781 64\ntorch.Size([49984])\ntensor([2.5582e-02, 3.5899e-04, 5.8519e-03,  ..., 9.2551e-01, 2.2986e-01,\n        1.3537e+00])\nMinimal probability 1.1159877789555003e-06, maximal probability 8.863830172651895e-05\n781 64\ntorch.Size([49984])\ntensor([3.3414e-02, 1.2802e-04, 1.2731e-02,  ..., 4.1161e-02, 1.1736e-02,\n        3.1298e+00])\nMinimal probability 1.2211056110374244e-06, maximal probability 8.644028995457406e-05\n781 64\ntorch.Size([49984])\ntensor([3.2140e-02, 3.4212e-05, 4.2000e-03,  ..., 4.7191e-01, 4.2927e-02,\n        2.0009e+00])\nMinimal probability 1.3354063089984636e-06, maximal probability 8.425145546647586e-05\n781 64\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader, loss_fn)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc}\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m callback\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[16], line 23\u001b[0m, in \u001b[0;36mtrain_full\u001b[0;34m(model, train_dataloader, loss_fn, optimizer, n_epochs, eval, callback)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_indices), \u001b[38;5;28mlen\u001b[39m(batch_indices[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_indices)):\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain_batch_is\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maccum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mtrain_batch_is\u001b[0;34m(model, x_batch, y_batch, loss_fn, optimizer, accumulator)\u001b[0m\n\u001b[1;32m     21\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     24\u001b[0m     batch_acc_sum \u001b[38;5;241m=\u001b[39m (output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39mn\n\u001b[1;32m     26\u001b[0m accumulator\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m     27\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m ( batch_loss, n),\n\u001b[1;32m     28\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m ( batch_acc_sum, n),\n\u001b[1;32m     29\u001b[0m     train_uniform_cnt \u001b[38;5;241m=\u001b[39m flag)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}